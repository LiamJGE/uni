{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "------------------------------------\n",
      "Accuracy: 75.52       Standard Deviation 4.28\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression:\n",
      "------------------------------------\n",
      "Accuracy: 76.95       Standard Deviation 4.84\n",
      "\n",
      "\n",
      "Stats Analysis\n",
      "---------------------------------------------\n",
      "\n",
      "Ttest_indResult(statistic=-2.375932736085734, pvalue=0.0176268572335954)\n",
      "\n",
      "p-value on its own : 0.0176268572335954\n",
      "\n",
      "Results are statistically significant\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from pandas import read_csv\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn 10FCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# SK learn Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Folds and seed\n",
    "num_folds = 10\n",
    "seed = 1\n",
    "\n",
    "print(\"Naive Bayes:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results.std()*100.0,2))\n",
    "\n",
    "# over all confusion matrix\n",
    "y_pred1 = cross_val_predict(model, X, Y, cv=kfold)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nLogistic Regression:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model2 = LogisticRegression(solver='liblinear')\n",
    "results2 = cross_val_score(model2, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results2.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results2.std()*100.0,2))\n",
    "\n",
    "# over all confusion matrix\n",
    "y_pred2 = cross_val_predict(model2, X, Y, cv=kfold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Statistics for output 1\n",
    "mean1 = round(results.mean()*100.0,2)\n",
    "\n",
    "# Binomial Theorem\n",
    "std1 = math.sqrt((results.mean()*len(y_pred1))*(1 - results.mean()))\n",
    "n1 = len(y_pred1)\n",
    "\n",
    "\n",
    "# Statistics for output 2\n",
    "mean2 = round(results2.mean()*100.0,2)\n",
    "\n",
    "# Bionomial Theorem\n",
    "std2 = math.sqrt((results2.mean()*len(y_pred2))*(1 - results2.mean()))\n",
    "n2 = len(y_pred2) \n",
    "\n",
    "# T-test must be for indeopendant means (as you are providing two)\n",
    "result = ttest_ind_from_stats(mean1, std1, n1, mean2, std2, n2)\n",
    "\n",
    "\n",
    "print(\"\\n\\nStats Analysis\\n---------------------------------------------\")\n",
    "print()\n",
    "print(result)\n",
    "print()\n",
    "print(\"p-value on its own :\", result[1])\n",
    "print()\n",
    "if result[1] < 0.05:\n",
    "    print(\"Results are statistically significant\")\n",
    "else:\n",
    "    print(\"Results are not statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we can see, the p-value is less than 0.05, therefore we reject the null hypothesis. Because there is a statistically significance between the two algorithms, we would choose the one with a higher accuracy which, in this case, is Logistic Regression</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "------------------------------------\n",
      "Accuracy: 75.52       Standard Deviation 4.28\n",
      "Mean:  75.52\n",
      "STD:  11.916004680331053\n",
      "N:  768\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression:\n",
      "------------------------------------\n",
      "Accuracy: 76.95       Standard Deviation 4.84\n",
      "Mean:  76.95\n",
      "STD:  11.671070407881018\n",
      "N:  768\n",
      "\n",
      "\n",
      "\n",
      "KNN:\n",
      "------------------------------------\n",
      "Accuracy: 70.56       Standard Deviation 6.06\n",
      "Mean:  70.56\n",
      "STD:  12.63067857706682\n",
      "N:  768\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree:\n",
      "------------------------------------\n",
      "Accuracy: 68.87       Standard Deviation 5.58\n",
      "Mean:  68.87\n",
      "STD:  12.831676705127537\n",
      "N:  768\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from pandas import read_csv\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn 10FCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# SK learn Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# Folds and seed\n",
    "num_folds = 10\n",
    "seed = 1\n",
    "\n",
    "print(\"Naive Bayes:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results.std()*100.0,2))\n",
    "y_pred1 = cross_val_predict(model, X, Y, cv=kfold)\n",
    "# Statistics for output 1\n",
    "mean1 = round(results.mean()*100.0,2)\n",
    "# Binomial Theorem\n",
    "std1 = math.sqrt((results.mean()*len(y_pred1))*(1 - results.mean()))\n",
    "n1 = len(y_pred1)\n",
    "print('Mean: ', mean1)\n",
    "print('STD: ', std1)\n",
    "print('N: ', n1)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nLogistic Regression:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model2 = LogisticRegression(solver='liblinear')\n",
    "results2 = cross_val_score(model2, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results2.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results2.std()*100.0,2))\n",
    "y_pred2 = cross_val_predict(model2, X, Y, cv=kfold)\n",
    "# Statistics for output 2\n",
    "mean2 = round(results2.mean()*100.0,2)\n",
    "# Bionomial Theorem\n",
    "std2 = math.sqrt((results2.mean()*len(y_pred2))*(1 - results2.mean()))\n",
    "n2 = len(y_pred2)\n",
    "print('Mean: ', mean2)\n",
    "print('STD: ', std2)\n",
    "print('N: ', n2)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nKNN:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "results3 = cross_val_score(model3, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results3.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results3.std()*100.0,2))\n",
    "y_pred3 = cross_val_predict(model3, X, Y, cv=kfold)\n",
    "# Statistics for output 3\n",
    "mean3 = round(results3.mean()*100.0,2)\n",
    "# Binomial Theorem\n",
    "std3 = math.sqrt((results3.mean()*len(y_pred3))*(1 - results3.mean()))\n",
    "n3 = len(y_pred3)\n",
    "print('Mean: ', mean3)\n",
    "print('STD: ', std3)\n",
    "print('N: ', n3)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nDecision Tree:\\n------------------------------------\")\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model4 = DecisionTreeClassifier()\n",
    "results4 = cross_val_score(model4, X, Y, cv=kfold)\n",
    "print(\"Accuracy:\", round(results4.mean()*100.0,2),\n",
    "      \"      Standard Deviation\", round(results4.std()*100.0,2))\n",
    "y_pred4 = cross_val_predict(model4, X, Y, cv=kfold)\n",
    "# Statistics for output 4\n",
    "mean4 = round(results4.mean()*100.0,2)\n",
    "# Bionomial Theorem\n",
    "std4 = math.sqrt((results4.mean()*len(y_pred4))*(1 - results4.mean()))\n",
    "n4 = len(y_pred4)\n",
    "print('Mean: ', mean4)\n",
    "print('STD: ', std4)\n",
    "print('N: ', n4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ANOVA and post-hoc test</h4>\n",
    "\n",
    "<p>Tukey HSD Post-hoc Test...</p>\n",
    "<p>Logistic Regression vs Naive Bayes: Diff=1.4300, 95%CI=-0.1791 to 3.0391, p=0.1020</p>\n",
    "<p>Logistic Regression vs KNN: Diff=-4.9600, 95%CI=-6.5691 to -3.3509, p=0.0000</p>\n",
    "<p>Logistic Regression vs Decision Tree: Diff=-6.6500, 95%CI=-8.2591 to -5.0409, p=0.0000</p>\n",
    "<p>Naive Bayes vs KNN: Diff=-6.3900, 95%CI=-7.9991 to -4.7809, p=0.0000</p>\n",
    "<p>Naive Bayes vs Decision Tree: Diff=-8.0800, 95%CI=-9.6891 to -6.4709, p=0.0000</p>\n",
    "<p>KNN vs Decision Tree: Diff=-1.6900, 95%CI=-3.2991 to -0.0809, p=0.0351</p>\n",
    "<br>\n",
    "<p>From the results obtained, we can say that there is no statistically significant difference between Logistic Regression and Naive Bayes. Therefore, we can choose any of these two algorithms depending on other factors like efficiency. For the rest of the p-values, there is a statistically significant difference due to the p-value being less than 0.05. This means that we should choose the algorithm with the highest accuracy.</p>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
